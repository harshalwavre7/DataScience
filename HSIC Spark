from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr, mean, sqrt, lit, exp, percentile_approx, first, udf
from pyspark.sql.types import DoubleType
from scipy.stats import gamma

spark = SparkSession.builder.appName("HSIC_PySpark").config("spark.sql.shuffle.partitions", "200").getOrCreate()

def compute_rbf_kernel(df1, df2, col_name, width_value):
    H = df1.alias("a").join(df2.alias("b"), how="cross")
    H = H.withColumn("diff", (col(f"a.{col_name}") - col(f"b.{col_name}")) ** 2)
    H = H.withColumn("H", exp(-col("diff") / (lit(2) * width_value * width_value)))
    return H.select("H")

def hsic_gam(df_X, df_Y, alph=0.5):
    n = df_X.count()
    
    width_x_df = df_X.alias("a").join(df_X.alias("b"), how="cross") \
        .selectExpr("(a.X - b.X) * (a.X - b.X) as dist")
    width_x_value = width_x_df.agg(expr("percentile_approx(dist, 0.5, 1000)").alias("width_x"))
    width_x_value = width_x_value.withColumn("width_x", sqrt(lit(0.5) * col("width_x"))).select("width_x")
    width_x_value = width_x_value.first()[0]  # Extract scalar value
    
    width_y_df = df_Y.alias("a").join(df_Y.alias("b"), how="cross") \
        .selectExpr("(a.Y - b.Y) * (a.Y - b.Y) as dist")
    width_y_value = width_y_df.agg(expr("percentile_approx(dist, 0.5, 1000)").alias("width_y"))
    width_y_value = width_y_value.withColumn("width_y", sqrt(lit(0.5) * col("width_y"))).select("width_y")
    width_y_value = width_y_value.first()[0]  # Extract scalar value
    
    K = compute_rbf_kernel(df_X, df_X, "X", width_x_value).alias("k")
    L = compute_rbf_kernel(df_Y, df_Y, "Y", width_y_value).alias("l")
    
    testStat = K.join(L, how="cross").agg(mean(col("k.H") * col("l.H")).alias("testStat"))
    
    varHSIC = K.join(L, how="cross") \
        .withColumn("var", (col("k.H") * col("l.H") / 6) ** 2) \
        .agg(mean("var").alias("varHSIC"))
    
    varHSIC = varHSIC.withColumn(
        "varHSIC",
        col("varHSIC") * (72 * (n - 4) * (n - 5) / (n * (n - 1) * (n - 2) * (n - 3)))
    )
    
    muX = K.agg(mean("H").alias("muX"))
    muY = L.agg(mean("H").alias("muY"))
    
    mHSIC = muX.crossJoin(muY).withColumn("mHSIC", (lit(1) + col("muX") * col("muY") - col("muX") - col("muY")) / lit(n)).select("mHSIC")
    
    al = varHSIC.crossJoin(mHSIC).withColumn("al", (col("mHSIC") ** 2) / col("varHSIC")).select("al")
    bet = varHSIC.crossJoin(mHSIC).withColumn("bet", (col("varHSIC") * lit(n)) / col("mHSIC")).select("bet")
    
    gamma_ppf_udf = udf(lambda a, b: float(gamma.ppf(1 - alph, a, scale=b)), DoubleType())
    thresh_df = al.crossJoin(bet).withColumn("thresh", gamma_ppf_udf(col("al"), col("bet")))
    
    thresh = thresh_df.select("thresh").first()[0]
    
    return testStat, thresh

# Example usage with PySpark DataFrames
df_X = spark.read.parquet("X_data.parquet").repartition(200)  # Optimized partitioning
df_Y = spark.read.parquet("Y_data.parquet").repartition(200)  # Optimized partitioning

test_stat, threshold = hsic_gam(df_X, df_Y)
print("HSIC Test Statistic:", test_stat)
print("Threshold:", threshold)
